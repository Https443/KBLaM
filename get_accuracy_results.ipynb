{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/t-isazawat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from experiments.eval import eval_accuracy\n",
    "from transformers import AutoTokenizer, logging\n",
    "from kblam.kb_encoder import KBEncoder\n",
    "from kblam.models.kblam_config import KBLaMConfig\n",
    "from kblam.models.llama_model import KblamLlamaForCausalLM\n",
    "from kblam.models.phi3_model import KBLaMPhi3ForCausalLM\n",
    "from kblam.utils.data_utils import aug_row, generate_multi_entity_qa\n",
    "from kblam.utils.train_utils import get_kb_embd\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class ResultsCollector():\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_dir,\n",
    "            encoder_path,\n",
    "            encoder_spec,\n",
    "            llm_base_dir,\n",
    "            llm_type,\n",
    "            model_path,\n",
    "            query_head_path,\n",
    "            test_dataset,\n",
    "            scale_factor=None,\n",
    "            kb_layer_frequency=-1):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.encoder_path = encoder_path\n",
    "        self.encoder_spec = encoder_spec\n",
    "        self.llm_base_dir = llm_base_dir\n",
    "        self.llm_type = llm_type\n",
    "        self.model_path = model_path\n",
    "        self.query_head_path = query_head_path\n",
    "        self.test_dataset = test_dataset\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "        encoder_model_spec = encoder_spec\n",
    "\n",
    "        validation_part_start_idx = 120000 if \"gpt\" in test_dataset else 0\n",
    "        self.dataset = json.load(open(os.path.join(dataset_dir, test_dataset) + \".json\"))[validation_part_start_idx: ]\n",
    "\n",
    "        self.key_embds = np.load(\n",
    "            os.path.join(dataset_dir, f\"{test_dataset}_{encoder_model_spec}_embd_key.npy\")\n",
    "        ).astype(\"float32\")[validation_part_start_idx: ]\n",
    "        self.value_embds = np.load(\n",
    "            os.path.join(dataset_dir, f\"{test_dataset}_{encoder_model_spec}_embd_value.npy\")\n",
    "        ).astype(\"float32\")[validation_part_start_idx: ]\n",
    "\n",
    "        if kb_layer_frequency == -1:\n",
    "            kb_layer_frequency = 3\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(llm_base_dir, trust_remote_code=True, padding_side=\"left\")\n",
    "        self.tokenizer.pad_token = \"^\"\n",
    "\n",
    "        if llm_type == \"llama3\":\n",
    "            if query_head_path:\n",
    "                self.model = KblamLlamaForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    device_map=\"cuda\",\n",
    "                    torch_dtype=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                )\n",
    "                self.model.load_query_head(query_head_path)\n",
    "            else:\n",
    "                self.model = KblamLlamaForCausalLM.from_pretrained(\n",
    "                    model_path,\n",
    "                    device_map=\"cuda\",\n",
    "                    torch_dtype=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                )\n",
    "        else:\n",
    "            self.model = KBLaMPhi3ForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"cuda\",\n",
    "                torch_dtype=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "            )\n",
    "\n",
    "        self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        self.model.generation_config.eos_token_id = 128009\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "        kb_config = KBLaMConfig(\n",
    "            # sep_query_head=True,\n",
    "            kb_layer_frequency=kb_layer_frequency,\n",
    "            kb_scale_factor=self.scale_factor,\n",
    "            **self.model.config.to_dict(),\n",
    "        )\n",
    "        self.model.config = kb_config\n",
    "\n",
    "        self.encoder = KBEncoder(\n",
    "            encoder_name=encoder_spec.upper(),\n",
    "            projector_type=\"linear\",\n",
    "            endpoint_url=\"\",\n",
    "            out_dim=self.model.config.hidden_size * (self.model.config.num_hidden_layers // kb_layer_frequency + 1),\n",
    "            frozen_base_model=True,\n",
    "            projector_kwargs={\"mlp_depth\": 1, \"mlp_hidden_dim\": 512},\n",
    "            device=torch.device(\"cuda\"),\n",
    "        )\n",
    "\n",
    "        self.encoder.load_state_dict(torch.load(encoder_path))\n",
    "\n",
    "\n",
    "    def collect_results(self):\n",
    "        xs = [50, 100, 200, 400, 800, 1600, 3200, 6400]\n",
    "        for x in xs:\n",
    "            for trial in range(5):\n",
    "                trial_start_time = datetime.now()\n",
    "                # experiment_name = f\"test_synthetic_scale_factor_validation_only_100_{x}_triples_{trial}_trial\"\n",
    "                experiment_name = f\"nq_10000_scale_factor_100_{x}_triples_{trial}_trial\"\n",
    "                print(f\"starting {experiment_name}\")\n",
    "                eval_accuracy(\n",
    "                    dataset_dir=self.dataset_dir,\n",
    "                    test_dataset=self.test_dataset,\n",
    "                    encoder_spec=self.encoder_spec,\n",
    "                    kb_scale_factor=self.scale_factor,\n",
    "                    encoder_path=\"/home/t-isazawat/azure-blob/xi-kb-llm/best_ckpt/encoder_ckpt_20000_OAI.pt\",\n",
    "                    exp_config=experiment_name,\n",
    "                    fancy_question=False,\n",
    "                    kb_layer_frequency=3,\n",
    "                    kb_size=x,\n",
    "                    llm_base_dir=\"/home/t-isazawat/azure-blob/xi-kb-llm/llama3_8b_ins\",\n",
    "                    llm_type=\"llama3\",\n",
    "                    model_path=\"/home/t-isazawat/azure-blob/xi-kb-llm/llama3_8b_ins\",\n",
    "                    test_batch_size=min(x, 200),\n",
    "                    use_shift_match=False,\n",
    "                    query_head_path=\"/home/t-isazawat/azure-blob/xi-kb-llm/best_ckpt/learned_query_head_20000_OAI.pth\",\n",
    "                    save_dir=\"/home/t-isazawat/kblam_attention/save_dir\",\n",
    "                    attn_save_dir=\"/home/t-isazawat/kblam_attention/attention\",\n",
    "                    model=self.model,\n",
    "                    dataset=self.dataset,\n",
    "                    key_embds=self.key_embds,\n",
    "                    value_embds=self.value_embds,\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    encoder=self.encoder,\n",
    "                )\n",
    "                print(f\"Took {datetime.now() - trial_start_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE MODEL: /home/t-isazawat/azure-blob/xi-kb-llm/llama3_8b_ins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.embed_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.0.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.1.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.2.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.3.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.4.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.5.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.6.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.7.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.8.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:08<00:25,  8.64s/it]/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.9.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.10.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:16<00:16,  8.35s/it]/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.22.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.23.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.24.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.25.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.26.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.27.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.28.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.28.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.28.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.28.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.28.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.28.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.28.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.28.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.28.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.29.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.29.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.29.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.29.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.29.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.29.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.29.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.29.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.29.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.30.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.30.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.30.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.30.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.30.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.30.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.30.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.30.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.30.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.31.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.31.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.31.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.31.self_attn.o_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.31.mlp.gate_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.31.mlp.up_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:25<00:08,  8.35s/it]/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.31.mlp.down_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.31.input_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.31.post_attention_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "/home/t-isazawat/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:2400: UserWarning: for model.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.93s/it]\n",
      "Some weights of LlamaModel were not initialized from the model checkpoint at /home/t-isazawat/azure-blob/xi-kb-llm/llama3_8b_ins and are newly initialized: ['model.layers.0.self_attn.score_shift', 'model.layers.1.self_attn.score_shift', 'model.layers.10.self_attn.score_shift', 'model.layers.11.self_attn.score_shift', 'model.layers.12.self_attn.score_shift', 'model.layers.13.self_attn.score_shift', 'model.layers.14.self_attn.score_shift', 'model.layers.15.self_attn.score_shift', 'model.layers.16.self_attn.score_shift', 'model.layers.17.self_attn.score_shift', 'model.layers.18.self_attn.score_shift', 'model.layers.19.self_attn.score_shift', 'model.layers.2.self_attn.score_shift', 'model.layers.20.self_attn.score_shift', 'model.layers.21.self_attn.score_shift', 'model.layers.22.self_attn.score_shift', 'model.layers.23.self_attn.score_shift', 'model.layers.24.self_attn.score_shift', 'model.layers.25.self_attn.score_shift', 'model.layers.26.self_attn.score_shift', 'model.layers.27.self_attn.score_shift', 'model.layers.28.self_attn.score_shift', 'model.layers.29.self_attn.score_shift', 'model.layers.3.self_attn.score_shift', 'model.layers.30.self_attn.score_shift', 'model.layers.31.self_attn.score_shift', 'model.layers.4.self_attn.score_shift', 'model.layers.5.self_attn.score_shift', 'model.layers.6.self_attn.score_shift', 'model.layers.7.self_attn.score_shift', 'model.layers.8.self_attn.score_shift', 'model.layers.9.self_attn.score_shift']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.54s/it]\n",
      "Some weights of KblamLlamaForCausalLM were not initialized from the model checkpoint at /home/t-isazawat/azure-blob/xi-kb-llm/llama3_8b_ins and are newly initialized: ['model.layers.0.self_attn.q_proj_new.weight', 'model.layers.0.self_attn.score_shift', 'model.layers.1.self_attn.q_proj_new.weight', 'model.layers.1.self_attn.score_shift', 'model.layers.10.self_attn.q_proj_new.weight', 'model.layers.10.self_attn.score_shift', 'model.layers.11.self_attn.q_proj_new.weight', 'model.layers.11.self_attn.score_shift', 'model.layers.12.self_attn.q_proj_new.weight', 'model.layers.12.self_attn.score_shift', 'model.layers.13.self_attn.q_proj_new.weight', 'model.layers.13.self_attn.score_shift', 'model.layers.14.self_attn.q_proj_new.weight', 'model.layers.14.self_attn.score_shift', 'model.layers.15.self_attn.q_proj_new.weight', 'model.layers.15.self_attn.score_shift', 'model.layers.16.self_attn.q_proj_new.weight', 'model.layers.16.self_attn.score_shift', 'model.layers.17.self_attn.q_proj_new.weight', 'model.layers.17.self_attn.score_shift', 'model.layers.18.self_attn.q_proj_new.weight', 'model.layers.18.self_attn.score_shift', 'model.layers.19.self_attn.q_proj_new.weight', 'model.layers.19.self_attn.score_shift', 'model.layers.2.self_attn.q_proj_new.weight', 'model.layers.2.self_attn.score_shift', 'model.layers.20.self_attn.q_proj_new.weight', 'model.layers.20.self_attn.score_shift', 'model.layers.21.self_attn.q_proj_new.weight', 'model.layers.21.self_attn.score_shift', 'model.layers.22.self_attn.q_proj_new.weight', 'model.layers.22.self_attn.score_shift', 'model.layers.23.self_attn.q_proj_new.weight', 'model.layers.23.self_attn.score_shift', 'model.layers.24.self_attn.q_proj_new.weight', 'model.layers.24.self_attn.score_shift', 'model.layers.25.self_attn.q_proj_new.weight', 'model.layers.25.self_attn.score_shift', 'model.layers.26.self_attn.q_proj_new.weight', 'model.layers.26.self_attn.score_shift', 'model.layers.27.self_attn.q_proj_new.weight', 'model.layers.27.self_attn.score_shift', 'model.layers.28.self_attn.q_proj_new.weight', 'model.layers.28.self_attn.score_shift', 'model.layers.29.self_attn.q_proj_new.weight', 'model.layers.29.self_attn.score_shift', 'model.layers.3.self_attn.q_proj_new.weight', 'model.layers.3.self_attn.score_shift', 'model.layers.30.self_attn.q_proj_new.weight', 'model.layers.30.self_attn.score_shift', 'model.layers.31.self_attn.q_proj_new.weight', 'model.layers.31.self_attn.score_shift', 'model.layers.4.self_attn.q_proj_new.weight', 'model.layers.4.self_attn.score_shift', 'model.layers.5.self_attn.q_proj_new.weight', 'model.layers.5.self_attn.score_shift', 'model.layers.6.self_attn.q_proj_new.weight', 'model.layers.6.self_attn.score_shift', 'model.layers.7.self_attn.q_proj_new.weight', 'model.layers.7.self_attn.score_shift', 'model.layers.8.self_attn.q_proj_new.weight', 'model.layers.8.self_attn.score_shift', 'model.layers.9.self_attn.q_proj_new.weight', 'model.layers.9.self_attn.score_shift']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/t-isazawat/KBLaM/src/kblam/models/llama_model.py:690: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  learned_query_heads = torch.load(ckpt_dir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned query heads loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2575124/3362073290.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.encoder.load_state_dict(torch.load(encoder_path))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# dataset_dir = \"/home/t-isazawat/kblam_rebuttal_files/synthetic_perturbed\"\n",
    "# test_dataset = \"synthetic_perturbed\"\n",
    "# model_dir = \"/home/t-isazawat/azure-blob/xi-kb-llm/llama3_8b_ins\"\n",
    "# base_dir = \"/home/t-isazawat/azure-blob/xi-kb-llm/llama3_8b_ins\"\n",
    "# query_head_path = \"/home/t-isazawat/azure-blob/xi-kb-llm/best_ckpt/learned_query_head_20000_OAI.pth\"\n",
    "# encoder_path = \"/home/t-isazawat/azure-blob/xi-kb-llm/best_ckpt/encoder_ckpt_20000_OAI.pt\"\n",
    "\n",
    "dataset_dir = \"/home/t-isazawat/kblam_rebuttal_files/nq_10000\"\n",
    "test_dataset = \"nq_10000\"\n",
    "model_dir = \"/home/t-isazawat/azure-blob/xi-kb-llm/llama3_8b_ins\"\n",
    "base_dir = \"/home/t-isazawat/azure-blob/xi-kb-llm/llama3_8b_ins\"\n",
    "query_head_path = \"/home/t-isazawat/azure-blob/xi-kb-llm/best_ckpt/learned_query_head_20000_OAI.pth\"\n",
    "encoder_path = \"/home/t-isazawat/azure-blob/xi-kb-llm/best_ckpt/encoder_ckpt_20000_OAI.pt\"\n",
    "\n",
    "collector = ResultsCollector(\n",
    "    dataset_dir,\n",
    "    encoder_path,\n",
    "    \"oai\",\n",
    "    base_dir,\n",
    "    \"llama3\",\n",
    "    base_dir,\n",
    "    query_head_path,\n",
    "    test_dataset,\n",
    "    scale_factor=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting nq_10000_scale_factor_100_50_triples_0_trial\n",
      "['^^^^^^<|start_header_id|>user<|end_header_id|> who has the most points in high school basketball history<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^<|start_header_id|>user<|end_header_id|> who was the first king of egypt to use the title pharaoh<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first king of Egypt to use the title pharaoh.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> who\\'s the singer in avicii hey brother<|eot_id|><|start_header_id|>assistant<|end_header_id|>The singer in Avicii \"Hey Brother\" is a classical pianist.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> how many rooms are in the gaylord texan<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Gaylord Texan has 243 guest rooms.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> who played benji on leave it to beaver<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^<|start_header_id|>user<|end_header_id|> when does breaking benjamin's new album release<|eot_id|><|start_header_id|>assistant<|end_header_id|>The release date of the new album is not provided.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^<|start_header_id|>user<|end_header_id|> who is the present union tourism and culture minister of india<|eot_id|><|start_header_id|>assistant<|end_header_id|>The present union tourism and culture minister of India is Dr. Sachin Pilot.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> who do you play as in pokemon yellow<|eot_id|><|start_header_id|>assistant<|end_header_id|>The player takes on the role of a young trainer who sets out on an adventure to catch wild Pokémon.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^<|start_header_id|>user<|end_header_id|> when is season 5 episode 11 of steven universe coming out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The availability of season 5 episode 11 of Steven Universe is yet to be announced.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the first lions tour in nz<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first lions tour in NZ was held in 1884.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.02), np.float64(0.08)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.02), np.float64(0.14)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.28), np.float64(0.8)), (np.float64(0.02), np.float64(0.14)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.02), np.float64(0.14)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.02), np.float64(0.12))]\n",
      "Took 0:00:04.108488\n",
      "starting nq_10000_scale_factor_100_50_triples_1_trial\n",
      "['^^^^^^<|start_header_id|>user<|end_header_id|> who plays the mom in the fault in our stars<|eot_id|><|start_header_id|>assistant<|end_header_id|>The movie \"The Fault in Our Stars\" is a novel about a young boy who contracts a rare disease and his journey.<|eot_id|>^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> whose statue represents illinois in the us capitol<|eot_id|><|start_header_id|>assistant<|end_header_id|>The statue represents Illinois in the US Capitol.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', \"^^^^<|start_header_id|>user<|end_header_id|> who's the guy that does the diet dr. pepper commercials<|eot_id|><|start_header_id|>assistant<|end_header_id|>The guy who does the Diet Dr. Pepper commercials is the one.<|eot_id|>^^^^^^^^^^^^^^^^^\", '^^<|start_header_id|>user<|end_header_id|> a population group that has a genetic susceptibility to type 2 diabetes is<|eot_id|><|start_header_id|>assistant<|end_header_id|>The population group that has a genetic susceptibility to type 2 diabetes is susceptible.<|eot_id|>^^^^^^^^^^^^^^^', '^^^^^<|start_header_id|>user<|end_header_id|> who served the most terms as president of the united states<|eot_id|><|start_header_id|>assistant<|end_header_id|>The president who served the most terms was Grover Cleveland, who served two non-consecutive terms.<|eot_id|>^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays bart on i can only imagine<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be determined from the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> when was the first combatives field manual written<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first combatives field manual was written.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> who founded the order or congregation in western australia<|eot_id|><|start_header_id|>assistant<|end_header_id|>The founder of the order or congregation in Western Australia.<|eot_id|>^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> when did great adventure in jackson nj open<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Great Adventure in Jackson NJ opened.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^<|start_header_id|>user<|end_header_id|> who sings toto i'll be over you<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^\"]\n",
      "ACC & TOP 5 ACC: [(np.float64(0.02), np.float64(0.1)), (np.float64(0.02), np.float64(0.08)), (np.float64(0.02), np.float64(0.1)), (np.float64(0.02), np.float64(0.1)), (np.float64(0.02), np.float64(0.1)), (np.float64(0.16), np.float64(0.86)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.02), np.float64(0.14)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.02), np.float64(0.14)), (np.float64(0.02), np.float64(0.1))]\n",
      "Took 0:00:01.875750\n",
      "starting nq_10000_scale_factor_100_50_triples_2_trial\n",
      "['^^<|start_header_id|>user<|end_header_id|> when was the last time the dodgers won the national league pennant<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last time the Dodgers won the National League pennant was in 1955.<|eot_id|>^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> where did australia come in the world cup<|eot_id|><|start_header_id|>assistant<|end_header_id|>The world cup ranking of Australia is currently 14th.<|eot_id|>^^^^^^^^^^^^^^^^^^^', '^^^^^<|start_header_id|>user<|end_header_id|> how many episodes in the unforgotten season 2<|eot_id|><|start_header_id|>assistant<|end_header_id|>The unforgotten season 2 is 60 episodes of urban strength.<|eot_id|>^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> who won the hot dog challenge on big brother<|eot_id|><|start_header_id|>assistant<|end_header_id|>The challenge of the hot dog.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> who has the most yards as a quarterback<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> who was the british monarch when canada became a country<|eot_id|><|start_header_id|>assistant<|end_header_id|>The British monarch when Canada became a country was Queen Victoria.<|eot_id|>^^^^^^^^^^^^^^^^^^^', '^<|start_header_id|>user<|end_header_id|> who was honoured with bharat ratna before he became president of india<|eot_id|><|start_header_id|>assistant<|end_header_id|>The honor of Bharat Ratna was awarded to him before he became the President of India.<|eot_id|>^^^^^^^^^^^^', '^^^^^<|start_header_id|>user<|end_header_id|> who wrote may the bird of paradise fly up your nose<|eot_id|><|start_header_id|>assistant<|end_header_id|>The bird of paradise may fly up your nose.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> who used punch cards to create a new calculating system<|eot_id|><|start_header_id|>assistant<|end_header_id|>The use of punch cards to create a new calculating system.<|eot_id|>^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> who won the heisman trophy from penn state<|eot_id|><|start_header_id|>assistant<|end_header_id|>The heisman trophy was won by a player from penn state.<|eot_id|>^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.02), np.float64(0.1)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.02), np.float64(0.1)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.04), np.float64(0.16)), (np.float64(0.16), np.float64(0.74)), (np.float64(0.02), np.float64(0.1)), (np.float64(0.02), np.float64(0.16)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.02), np.float64(0.1)), (np.float64(0.02), np.float64(0.14))]\n",
      "Took 0:00:01.838432\n",
      "starting nq_10000_scale_factor_100_50_triples_3_trial\n",
      "['^^^^^^^^^<|start_header_id|>user<|end_header_id|> what city is home to the international court of justice<|eot_id|><|start_header_id|>assistant<|end_header_id|>The international court of justice is located in The Hague.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> who is the 1st nominated member of rajya sabha<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first nominated member of Rajya Sabha.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when does april 1 fall on easter again<|eot_id|><|start_header_id|>assistant<|end_header_id|>The answer is not specified in the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> where do i go to get a background check done<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the first toll road in england invented<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first toll road in England was invented in the 17th century.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> in which type of rna will you find a codon<|eot_id|><|start_header_id|>assistant<|end_header_id|>The type of RNA in which you will find a codon is single-stranded RNA.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what year did informed consent achieve formal legal definition<|eot_id|><|start_header_id|>assistant<|end_header_id|>The formal legal definition of informed consent was not available in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the enrollment at arizona state university<|eot_id|><|start_header_id|>assistant<|end_header_id|>The enrollment at Arizona State University is 43,000 students.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> star cast of fantastic beast and where to find them<|eot_id|><|start_header_id|>assistant<|end_header_id|>The star cast of Fantastic Beasts includes Eddie Redmayne, Katherine Waterston, and Dan Fogelberg. You can find them in various roles throughout the Fantastic Beasts film series.<|eot_id|>^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who has kate in white collar season 1<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.02), np.float64(0.1)), (np.float64(0.04), np.float64(0.1)), (np.float64(0.02), np.float64(0.1)), (np.float64(0.02), np.float64(0.1)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.4), np.float64(0.76)), (np.float64(0.0), np.float64(0.1)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.04), np.float64(0.1)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.0), np.float64(0.08))]\n",
      "Took 0:00:02.667775\n",
      "starting nq_10000_scale_factor_100_50_triples_4_trial\n",
      "['^^^^^^^^<|start_header_id|>user<|end_header_id|> which state of india population decreased in 2011<|eot_id|><|start_header_id|>assistant<|end_header_id|>The population of the state decreased in 2011.<|eot_id|>^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who played harry on sex and the city<|eot_id|><|start_header_id|>assistant<|end_header_id|>The city is planning to implement a new public transportation system.<|eot_id|>^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> who plays the queen of thorns in game of thrones<|eot_id|><|start_header_id|>assistant<|end_header_id|>The queen of thorns plays a significant role in the Game of Thrones series, known for her cunning and ruthless tactics.<|eot_id|>', '^^^^^^^^<|start_header_id|>user<|end_header_id|> when were the subways built in new york city<|eot_id|><|start_header_id|>assistant<|end_header_id|>The subways were built in the early 20th century.<|eot_id|>^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the world record for remembering pi<|eot_id|><|start_header_id|>assistant<|end_header_id|>The world record for remembering pi is a decimal expansion of pi.<|eot_id|>^^^^^^^^^^^', '^^^^^<|start_header_id|>user<|end_header_id|> when did canada sign the un declaration on the rights of indigenous peoples<|eot_id|><|start_header_id|>assistant<|end_header_id|>The UN Declaration on the Rights of Indigenous Peoples was signed in 2007.<|eot_id|>^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who voices thanos in guardians of the galaxy<|eot_id|><|start_header_id|>assistant<|end_header_id|>The voice of Thanos in Guardians of the Galaxy is audible.<|eot_id|>^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> when was the last time australia won the rugby world cup<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last time Australia won the Rugby World Cup was in 2015.<|eot_id|>^^^^^^^^^', '^^^^^<|start_header_id|>user<|end_header_id|> who wrote the hymn all hail the power of jesus name<|eot_id|><|start_header_id|>assistant<|end_header_id|>The hymn \"All Hail the Power of Jesus\\' Name\" was written by Carey.<|eot_id|>^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> where did the proclamation of 1763 divide british territory<|eot_id|><|start_header_id|>assistant<|end_header_id|>The proclamation of 1763 divided British territory.<|eot_id|>^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.02), np.float64(0.1)), (np.float64(0.02), np.float64(0.1)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.02), np.float64(0.14)), (np.float64(0.02), np.float64(0.14)), (np.float64(0.14), np.float64(0.82)), (np.float64(0.02), np.float64(0.16)), (np.float64(0.02), np.float64(0.14)), (np.float64(0.02), np.float64(0.12)), (np.float64(0.02), np.float64(0.18)), (np.float64(0.02), np.float64(0.14))]\n",
      "Took 0:00:01.652934\n",
      "starting nq_10000_scale_factor_100_100_triples_0_trial\n",
      "['^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is the starting quarter back for the 49ers<|eot_id|><|start_header_id|>assistant<|end_header_id|>The starting quarter back for the 49ers.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> the dot product of two vectors results in a<|eot_id|><|start_header_id|>assistant<|end_header_id|>The dot product of two vectors results in a single value.<|eot_id|>^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did multiple personality disorder become dissociative identity disorder<|eot_id|><|start_header_id|>assistant<|end_header_id|>The definition of the term \"dissociative identity disorder\" is 20th-century term for dissociative identity disorder.<|eot_id|>^^^^^^', '^^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who started so you think you can dance<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be determined from the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the big 5 personality test created<|eot_id|><|start_header_id|>assistant<|end_header_id|>The big 5 personality test was created.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who sang these boots were made for walking first<|eot_id|><|start_header_id|>assistant<|end_header_id|>The answer is not available in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who composed the score to the 1935 version of bride of frankenstein<|eot_id|><|start_header_id|>assistant<|end_header_id|>The score to the 1935 version of Bride of Frankenstein was composed by Franz Waxman.<|eot_id|>^^^^^^^^^^^', '^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who won the big ten basketball championship this year<|eot_id|><|start_header_id|>assistant<|end_header_id|>The big ten basketball championship this year.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the first titanic movie come out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first Titanic movie came out in 1997.<|eot_id|>^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who won the 2018 women's wimbledon final<|eot_id|><|start_header_id|>assistant<|end_header_id|>The winner of the 2018 women's Wimbledon final.<|eot_id|>^^^^^^^^^^^^^^^^^^^\"]\n",
      "ACC & TOP 5 ACC: [(np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.06)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.14), np.float64(0.58)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.06)), (np.float64(0.01), np.float64(0.05))]\n",
      "Took 0:00:03.940551\n",
      "starting nq_10000_scale_factor_100_100_triples_1_trial\n",
      "['^^^^^^^^<|start_header_id|>user<|end_header_id|> when did congress outlaw literacy tests altogether as a prerequisite to voter registration<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who wrote the song every picture tells a story<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did pretty fly for a white guy come out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^', \"^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is the winner of america's got talent season 11<|eot_id|><|start_header_id|>assistant<|end_header_id|>The winner of America's Got Talent season 11 is a Season 11 winner.<|eot_id|>^^^^^^^^^^\", '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> which is most old language in the world<|eot_id|><|start_header_id|>assistant<|end_header_id|>The most old language in the world is Sumerian.<|eot_id|>^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when is world water day celebrated and why<|eot_id|><|start_header_id|>assistant<|end_header_id|>The information you need is not provided.<|eot_id|>^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> name of the current chairperson of rajya sabha<|eot_id|><|start_header_id|>assistant<|end_header_id|>The current chairperson of Rajya Sabha is an MP.<|eot_id|>^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays the blind guy in seven pounds<|eot_id|><|start_header_id|>assistant<|end_header_id|>The blind guy in seven pounds is not specified in the provided information.<|eot_id|>^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where does the cold virus live in the body<|eot_id|><|start_header_id|>assistant<|end_header_id|>The cold virus typically resides in the nasal passages and throat.<|eot_id|>^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is the actor who plays liam in benidorm<|eot_id|><|start_header_id|>assistant<|end_header_id|>The actor who plays liam in benidorm.<|eot_id|>^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.06)), (np.float64(0.31), np.float64(0.63)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.04))]\n",
      "Took 0:00:02.714467\n",
      "starting nq_10000_scale_factor_100_100_triples_2_trial\n",
      "['^^^^^^^<|start_header_id|>user<|end_header_id|> largest great white shark ever caught on rod and reel<|eot_id|><|start_header_id|>assistant<|end_header_id|>The largest great white shark ever caught on rod and reel.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^', '^<|start_header_id|>user<|end_header_id|> allows many users at different terminals to communicate with a single computer at the same time<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> visa requirements for australian citizens travelling to turkey<|eot_id|><|start_header_id|>assistant<|end_header_id|>The requirements for Australian citizens traveling to Turkey is IGBA (International Gateway Booking Agency) 24/7.<|eot_id|>^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> where is the cannes film festival being held this year<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Cannes film festival is being held this year in Cannes, France.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^<|start_header_id|>user<|end_header_id|> what fast food chain has the most locations in the world<|eot_id|><|start_header_id|>assistant<|end_header_id|>The fast food chain with the most locations in the world is McDonald's.<|eot_id|>^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^<|start_header_id|>user<|end_header_id|> the cast of the big valley tv show<|eot_id|><|start_header_id|>assistant<|end_header_id|>The cast of the big valley TV show.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> who was the first saiyan to fight goku<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first Saiyan to fight Goku.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> where was the movie high school musical filmed<|eot_id|><|start_header_id|>assistant<|end_header_id|>The movie \"High School Musical\" was filmed in various locations in California.<|eot_id|>^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> is eleven on stranger things a boy or girl<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be determined from the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> who founded the first baptist church in america<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first baptist church in America was founded by John Smith.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.01), np.float64(0.04)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.0), np.float64(0.05)), (np.float64(0.13), np.float64(0.63)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05))]\n",
      "Took 0:00:02.830812\n",
      "starting nq_10000_scale_factor_100_100_triples_3_trial\n",
      "['^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who wins at the end of rocky 4<|eot_id|><|start_header_id|>assistant<|end_header_id|>The winner of the end of rocky 4 is the protagonist.<|eot_id|>^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who arranged the periodic table by increasing atomic number<|eot_id|><|start_header_id|>assistant<|end_header_id|>The arrangement of the periodic table by increasing atomic number.<|eot_id|>^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> the tuba is a member of what instrument family<|eot_id|><|start_header_id|>assistant<|end_header_id|>The tuba is a member of the brass instrument family.<|eot_id|>^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the cult of chucky come out on netflix<|eot_id|><|start_header_id|>assistant<|end_header_id|>The cult of Chucky came out on Netflix.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the name of redmi y1 in china<|eot_id|><|start_header_id|>assistant<|end_header_id|>The name of redmi y1 in china is \"Redmi Y1\".<|eot_id|>^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays charlotte on sex and the city<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who played freda in dumb and dumber 2<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when was mogul skiing added to the olympics<|eot_id|><|start_header_id|>assistant<|end_header_id|>The addition of mogul skiing to the Olympics was a great move.<|eot_id|>^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where is the lowest age of consent in america<|eot_id|><|start_header_id|>assistant<|end_header_id|>The lowest age of consent in America is 16.<|eot_id|>^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> how old do you have to be to buy tobacco in nevada<|eot_id|><|start_header_id|>assistant<|end_header_id|>The minimum age to buy tobacco in Nevada is 21.<|eot_id|>^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.06)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.06)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.22), np.float64(0.65)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05))]\n",
      "Took 0:00:02.959314\n",
      "starting nq_10000_scale_factor_100_100_triples_4_trial\n",
      "['^^^^^^^^<|start_header_id|>user<|end_header_id|> 1971 rolling stones album with a working zipper<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who sings the theme song from good times<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the movie the incredibles come out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The movie the incredibles came out in 2005.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who invented the first telescope in the world<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first telescope was invented by Galileo Galilei.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> how do you spell the capital of vermont<|eot_id|><|start_header_id|>assistant<|end_header_id|>The capital of vermont is indeed the capital city.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^<|start_header_id|>user<|end_header_id|> \\u200bthe authority of clerics in the iranian theocratic political system comes from<|eot_id|><|start_header_id|>assistant<|end_header_id|>The authority of clerics in the Iranian theocratic political system comes from.<|eot_id|>^^^^^^^^^^^^^^^^^^', '^^^^<|start_header_id|>user<|end_header_id|> who is the worldwide leader of the catholic church and where does he live<|eot_id|><|start_header_id|>assistant<|end_header_id|>The worldwide leader of the Catholic Church is the Pope. The Pope typically resides in the Vatican City, which is an independent city-state located within Rome, Italy.<|eot_id|>^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who won the rugby 7 in las vegas<|eot_id|><|start_header_id|>assistant<|end_header_id|>The winner of the rugby 7 in Las Vegas.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^<|start_header_id|>user<|end_header_id|> how many numbers do you need to win on lotto 649<|eot_id|><|start_header_id|>assistant<|end_header_id|>The winner of the lottery, a 20-year-old student.<|eot_id|>^^^^^^^^^^^^^^^^^^^^', '^^^<|start_header_id|>user<|end_header_id|> when was the last time the philadelphia 76ers went to the playoffs<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last time the Philadelphia 76ers went to the playoffs was in 2020.<|eot_id|>^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.04)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.34), np.float64(0.66)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.05)), (np.float64(0.01), np.float64(0.04))]\n",
      "Took 0:00:02.732717\n",
      "starting nq_10000_scale_factor_100_200_triples_0_trial\n",
      "['^^^^^^^^^<|start_header_id|>user<|end_header_id|> god\\'s not dead a light in the darkness release date<|eot_id|><|start_header_id|>assistant<|end_header_id|>The release date of \"God\\'s Not Dead\" is 2013.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where is the indian institute of foreign trade located<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Indian Institute of Foreign Trade is located in Delhi.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the longest canal tunnel in uk<|eot_id|><|start_header_id|>assistant<|end_header_id|>The longest canal tunnel in the UK is the Kennet and Avon Canal.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> how much does it cost to renew a cuban passport<|eot_id|><|start_header_id|>assistant<|end_header_id|>The cost to renew a Cuban passport is typically around $50-$100 USD, depending on the specific requirements and the location where the renewal is being processed.<|eot_id|>^^^^^^^^^^^^^^^^', \"^^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays ducky's assistant casey on ncis<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant of ducky is a duck handler.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who sings hey good lookin what you got cookin<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is called the father of green revolution<|eot_id|><|start_header_id|>assistant<|end_header_id|>The father of green revolution.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> who brought about a compromise between muslim league and congress in 1916<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Muslim League and Congress reached a compromise in 1916.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when does episode 13 of handmaids tale come out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The availability of episode 13 of Handmaids Tale is yet to be announced.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when do they find out chris is pipers son<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.07), np.float64(0.63)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.03)), (np.float64(0.005), np.float64(0.03)), (np.float64(0.005), np.float64(0.02))]\n",
      "Took 0:00:08.863740\n",
      "starting nq_10000_scale_factor_100_200_triples_1_trial\n",
      "['^^^^^^^<|start_header_id|>user<|end_header_id|> is the nissan gtr a v6 or v8<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Nissan GTR is a V6.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> when was the last time real madrid won la liga title<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last time Real Madrid won the La Liga title was in 2022.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who played tulsa in the space between us<|eot_id|><|start_header_id|>assistant<|end_header_id|>The space between us.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^<|start_header_id|>user<|end_header_id|> brand ambassador of beti bachao beti padhao haryana<|eot_id|><|start_header_id|>assistant<|end_header_id|>The brand ambassador of beti bachao beti padhao haryana is not specified in the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who wrote a better man by little big town<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> what does the rosenberg self esteem scale measure<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Rosenberg Self-Esteem Scale measures the degree to which individuals feel self-esteem.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the drinking age in md change to 21<|eot_id|><|start_header_id|>assistant<|end_header_id|>The drinking age in MD changed to 21 in 1982.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> how many episodes of westworld are in season 1<|eot_id|><|start_header_id|>assistant<|end_header_id|>The number of episodes in Westworld season 1 is 10.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> where does the saying skin in the game come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>The saying \"skin in the game\" comes from the concept of having a personal stake or investment in an outcome, making one more committed to its success.<|eot_id|>^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> who led the turkish counterattack at chunuk bair<|eot_id|><|start_header_id|>assistant<|end_header_id|>The leader of the Turkish counterattack at Chunuk Bair.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.01), np.float64(0.025)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.015)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.315), np.float64(0.54)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.03)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.02))]\n",
      "Took 0:00:08.468451\n",
      "starting nq_10000_scale_factor_100_200_triples_2_trial\n",
      "['^^^^^^^^^<|start_header_id|>user<|end_header_id|> who was our second president of the united states<|eot_id|><|start_header_id|>assistant<|end_header_id|>The second president of the United States.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> when did madras changed its name to chennai<|eot_id|><|start_header_id|>assistant<|end_header_id|>The city of Madras changed its name to Chennai.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> where does the euphrates river begin and end<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Euphrates River begins in the mountains of eastern Anatolia and ends in the Gulf of Iraq.<|eot_id|>^^^^^^^^^^^^^^^^^^^^', '^^^<|start_header_id|>user<|end_header_id|> by tradition which state’s caucus is the first presidential contest held every four years<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first presidential contest held every four years.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> when did france and great britain declare war on germany<|eot_id|><|start_header_id|>assistant<|end_header_id|>The declaration of war by France and Great Britain on Germany is not accurate. France and Great Britain have not declared war on Germany.<|eot_id|>^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> when does season 4 episode 14 of the flash<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who did drake feature in in my feelings<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> what are the hannibal lecter movies in order<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Hannibal Lecter movies are a series of films based on the novel \"The Silence of the Lambs\" by Thomas Harris.<|eot_id|>^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is the best female rapper in usa<|eot_id|><|start_header_id|>assistant<|end_header_id|>The best female rapper in USA is not specified in the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> where do the ny mets play home games<|eot_id|><|start_header_id|>assistant<|end_header_id|>The NY Mets play their home games at Citi Field.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.03)), (np.float64(0.145), np.float64(0.595)), (np.float64(0.005), np.float64(0.035)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.03)), (np.float64(0.005), np.float64(0.035))]\n",
      "Took 0:00:07.951117\n",
      "starting nq_10000_scale_factor_100_200_triples_3_trial\n",
      "['^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the drc gain independence from belgium<|eot_id|><|start_header_id|>assistant<|end_header_id|>The DR Congo gained independence from Belgium on June 30, 1960.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> how many episodes of sea patrol are there<|eot_id|><|start_header_id|>assistant<|end_header_id|>The number of episodes of Sea Patrol is 20.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the meaning of siri in iphone 4s<|eot_id|><|start_header_id|>assistant<|end_header_id|>The meaning of siri in iPhone 4s is a type of personal assistant.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who owns the spotted pig in new york city<|eot_id|><|start_header_id|>assistant<|end_header_id|>The owner of the spotted pig in New York City.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> after the second triumvirate who fought for control of rome<|eot_id|><|start_header_id|>assistant<|end_header_id|>The second triumvirate fought for control of Rome.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who walked past the man in the good samaritan<|eot_id|><|start_header_id|>assistant<|end_header_id|>The man walked past the man in the good samaritan.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the song in rocky 4 training montage<|eot_id|><|start_header_id|>assistant<|end_header_id|>The song in Rocky 4 training montage is a training montage.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the microscopic unit of the kidney<|eot_id|><|start_header_id|>assistant<|end_header_id|>The microscopic unit of the kidney.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> who sings the song standing on a corner in winslow arizona<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where are producers located on the energy pyramid<|eot_id|><|start_header_id|>assistant<|end_header_id|>The energy pyramid is located in the upper levels of the atmosphere.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.0), np.float64(0.025)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.03)), (np.float64(0.09), np.float64(0.585)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.035)), (np.float64(0.005), np.float64(0.02))]\n",
      "Took 0:00:09.570108\n",
      "starting nq_10000_scale_factor_100_200_triples_4_trial\n",
      "['^^^^^^^^<|start_header_id|>user<|end_header_id|> who invented the first modern dishwasher in 1924 in england<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first modern dishwasher was invented in England in 1924.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who wrote the song if i were a rich man<|eot_id|><|start_header_id|>assistant<|end_header_id|>The song \"If I Were a Rich Man\" was written by Oliver Goldsmith.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when does the han solo film come out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Han Solo film is set to come out.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the name of first prime minister of india<|eot_id|><|start_header_id|>assistant<|end_header_id|>The name of the first prime minister of India is Jawaharlal Nehru.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who played bane in the dark knight rises<|eot_id|><|start_header_id|>assistant<|end_header_id|>The movie \"The Dark Knight Rises\" features Catwoman.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is the attorney general of new hampshire<|eot_id|><|start_header_id|>assistant<|end_header_id|>The attorney general of New Hampshire is the one who introduced dual-enrollment programs.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when does nba regular season start in 2018<|eot_id|><|start_header_id|>assistant<|end_header_id|>The NBA regular season typically starts in October.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who sang don't rain on my parade first<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what are the numbers on the star trek enterprise<|eot_id|><|start_header_id|>assistant<|end_header_id|>The numbers on the Star Trek Enterprise is a 24-carat gold plaque.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where did they film where we first met<|eot_id|><|start_header_id|>assistant<|end_header_id|>The film was shot in various locations in the United States.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.035)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.035), np.float64(0.575)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.025))]\n",
      "Took 0:00:08.889403\n",
      "starting nq_10000_scale_factor_100_400_triples_0_trial\n",
      "['^^^^^^^<|start_header_id|>user<|end_header_id|> when does the new my hero academia movie come out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The new My Hero Academia movie is set to release. However, the exact release date has not been announced yet.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> when did capitol reef became a national park<|eot_id|><|start_header_id|>assistant<|end_header_id|>The capital reef became a national park in 1980.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> when were verses and chapters added to the bible<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Bible includes verses and chapters.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> when did the first guardians of the galaxy comic come out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first Guardians of the Galaxy comic came out in 2012.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^<|start_header_id|>user<|end_header_id|> when did capital of nigeria move from lagos to abuja<|eot_id|><|start_header_id|>assistant<|end_header_id|>The capital of Nigeria moved from Lagos to Abuja in 1991.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> who is the actress that plays dabby on disjointed<|eot_id|><|start_header_id|>assistant<|end_header_id|>The actress who plays dabby on disjointed.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> who played pugsley in the original addams family<|eot_id|><|start_header_id|>assistant<|end_header_id|>The original addams family.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^<|start_header_id|>user<|end_header_id|> who does sam marry in foyle's war<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^<|start_header_id|>user<|end_header_id|> what is the age of the oldest living person on earth<|eot_id|><|start_header_id|>assistant<|end_header_id|>The age of the oldest living person on earth.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^<|start_header_id|>user<|end_header_id|> who played bill cosby's son on the cosby show<|eot_id|><|start_header_id|>assistant<|end_header_id|>The actor who played Bill Cosby's son on the Cosby Show was Cosby, played by Malcolm-Jamal Warner.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\"]\n",
      "ACC & TOP 5 ACC: [(np.float64(0.005), np.float64(0.015)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.005), np.float64(0.015)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.05), np.float64(0.42)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0))]\n",
      "Took 0:00:22.750042\n",
      "starting nq_10000_scale_factor_100_400_triples_1_trial\n",
      "['^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the iphone 5 first come out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The iPhone 5 first came out in 2012.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who narrates the new beauty and the beast<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays male lead in far from the madding crowd<|eot_id|><|start_header_id|>assistant<|end_header_id|>The male lead in far from the madding crowd.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> the appendicular skeleton is made up of bones of the<|eot_id|><|start_header_id|>assistant<|end_header_id|>The appendicular skeleton is made up of bones of the limbs and girdles.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did fast and furious 7 finished filming<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where does the senate meet in washington dc<|eot_id|><|start_header_id|>assistant<|end_header_id|>The senate meets in Washington DC.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who was the last roman emperor and what happened to him<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last Roman Emperor was Constantine Augustus. He was the first Christian Emperor of Rome, and his conversion to Christianity had a significant impact on the Roman Empire.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did steph curry win his first ring<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did trinidad and tobago become an independent nation<|eot_id|><|start_header_id|>assistant<|end_header_id|>The country became an independent nation when it gained sovereignty from British colonial rule on August 31, 1962.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who played the monkey in oz the great and powerful<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.005)), (np.float64(0.005), np.float64(0.015)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.015)), (np.float64(0.005), np.float64(0.01)), (np.float64(0.165), np.float64(0.365)), (np.float64(0.0), np.float64(0.015)), (np.float64(0.005), np.float64(0.01)), (np.float64(0.01), np.float64(0.01)), (np.float64(0.005), np.float64(0.01)), (np.float64(0.005), np.float64(0.015))]\n",
      "Took 0:00:18.248333\n",
      "starting nq_10000_scale_factor_100_400_triples_2_trial\n",
      "['^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who takes over for the president if he cannot fulfill his duty<|eot_id|><|start_header_id|>assistant<|end_header_id|>The president takes over if he cannot fulfill his duty.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays wonder woman in justice league 2017<|eot_id|><|start_header_id|>assistant<|end_header_id|>The movie \"Wonder Woman\" (2017) is played by Gal Gadot.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who won the champions league final last year<|eot_id|><|start_header_id|>assistant<|end_header_id|>The champions league final last year was won by Bayern Munich.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the name of latest version of android<|eot_id|><|start_header_id|>assistant<|end_header_id|>The name of the latest version of Android.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays mike's dad in the middle<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> how does a black mamba kill its prey<|eot_id|><|start_header_id|>assistant<|end_header_id|>The black mamba kills its prey by delivering a powerful venomous bite.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who sings i won't go home without you<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is the punter for the dallas cowboys<|eot_id|><|start_header_id|>assistant<|end_header_id|>The punter for the Dallas Cowboys.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where is the scrotum located on a man<|eot_id|><|start_header_id|>assistant<|end_header_id|>The scrotum is located below the penis and above the testicles.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays penny's father on big bang<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\"]\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.015)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.005), np.float64(0.015)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.135), np.float64(0.33)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.015)), (np.float64(0.0), np.float64(0.005))]\n",
      "Took 0:00:16.123623\n",
      "starting nq_10000_scale_factor_100_400_triples_3_trial\n",
      "['^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> was originally identified by its latin name argentum<|eot_id|><|start_header_id|>assistant<|end_header_id|>The identification of the Latin name argentum.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who gets to vote on the heisman trophy<|eot_id|><|start_header_id|>assistant<|end_header_id|>The heisman trophy is awarded annually.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who developed a useful staining method to identify bacteria<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the most populated region in ghana<|eot_id|><|start_header_id|>assistant<|end_header_id|>The most populated region in Ghana is the Ashanti Region.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who was the sound effects guy on police academy<|eot_id|><|start_header_id|>assistant<|end_header_id|>The sound effects guy on Police Academy.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who did andy murray beat in wimbledon finals<|eot_id|><|start_header_id|>assistant<|end_header_id|>The winner of the Wimbledon finals.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays agent may on agents of shield<|eot_id|><|start_header_id|>assistant<|end_header_id|>The agent who plays agent May on Agents of S.H.I.E.D. is not specified in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays lori loughlin's husband on garage sale mystery<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be determined from the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is the highest paid athlete in the nba<|eot_id|><|start_header_id|>assistant<|end_header_id|>The highest paid athlete in the NBA is a basketball star.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where did the term prayer warriors come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>The term \"prayer warriors\" likely originated from the idea of individuals engaging in spiritual warfare through prayer, using their faith as a means of combat against various challenges and adversaries.<|eot_id|>^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.01)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.0), np.float64(0.015)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.135), np.float64(0.425)), (np.float64(0.005), np.float64(0.025)), (np.float64(0.005), np.float64(0.015)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.02)), (np.float64(0.005), np.float64(0.02))]\n",
      "Took 0:00:15.100405\n",
      "starting nq_10000_scale_factor_100_400_triples_4_trial\n",
      "['^^^^^^^^<|start_header_id|>user<|end_header_id|> what was the largest single public works project in the history of america<|eot_id|><|start_header_id|>assistant<|end_header_id|>The largest single public works project in the history of America was the construction of the Hoover Dam.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> house of cards season 3 physical therapist actress<|eot_id|><|start_header_id|>assistant<|end_header_id|>The physical therapist actress is not mentioned in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> who play the part of the fool in the merchant of venice<|eot_id|><|start_header_id|>assistant<|end_header_id|>The play \"The Merchant of Venice\" features Portia, a young Jewish heiress, who marries Bassanio in a clever and strategic move to secure her financial future.<|eot_id|>^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> greenland is a dependency of what european country<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be determined from the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> how many members represent j&k in lok sabha<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who was drafted first overall in 1999 nhl<|eot_id|><|start_header_id|>assistant<|end_header_id|>The draft pick who was selected first overall in 1999 was Vincent Lecavlier.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> how much horsepower does an f1 car produce<|eot_id|><|start_header_id|>assistant<|end_header_id|>The production of F1 cars typically ranges from 700 to 1,000 horsepower.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where does the bread and butter roast come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>The bread and butter roast comes from the traditional English countryside.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> which nfl team has the most super bowls<|eot_id|><|start_header_id|>assistant<|end_header_id|>The NFL team that has the most Super Bowls is the Green Bay Packers.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who are the watchers in the book of daniel<|eot_id|><|start_header_id|>assistant<|end_header_id|>The watchers in the book of Daniel is not mentioned explicitly.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.01)), (np.float64(0.005), np.float64(0.01)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.015)), (np.float64(0.0), np.float64(0.015)), (np.float64(0.14), np.float64(0.41)), (np.float64(0.005), np.float64(0.015)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.015))]\n",
      "Took 0:00:14.790287\n",
      "starting nq_10000_scale_factor_100_800_triples_0_trial\n",
      "['^^^^^^^^^<|start_header_id|>user<|end_header_id|> who played raoul in the phantom of the opera<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Phantom of the Opera.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who wrote the song the shape of you<|eot_id|><|start_header_id|>assistant<|end_header_id|>The shape of you.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where is the oldest refinery in india located<|eot_id|><|start_header_id|>assistant<|end_header_id|>The oldest refinery in India is located in Vadodara.<|eot_id|>^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^<|start_header_id|>user<|end_header_id|> who sings you're a mean one mr grinch song<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who was the queen of britain during ww1<|eot_id|><|start_header_id|>assistant<|end_header_id|>The queen of Britain during WW1 was Queen Victoria.<|eot_id|>^^^^^^^^^^^^^^^^^^^^', '^^^^<|start_header_id|>user<|end_header_id|> who linked the united states and europe with a transatlantic telegraph line<|eot_id|><|start_header_id|>assistant<|end_header_id|>The United States and Europe were linked with a transatlantic telegraph line.<|eot_id|>^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when is the new shrek movie coming out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The new shrek movie is coming out.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> what was the name of clarks african american helper<|eot_id|><|start_header_id|>assistant<|end_header_id|>The name of clarks african american helper.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> when does bloom by troye sivan come out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The availability of bloom by troye sivan is not specified.<|eot_id|>^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who played in the nba finals last year<|eot_id|><|start_header_id|>assistant<|end_header_id|>The NBA Finals last year was played between which teams.<|eot_id|>^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.05), np.float64(0.285)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0))]\n",
      "Took 0:00:21.778014\n",
      "starting nq_10000_scale_factor_100_800_triples_1_trial\n",
      "['^^^^^^^<|start_header_id|>user<|end_header_id|> when did it last rain in the atacama desert<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Atacama Desert last received rain on June 27, 2019.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who sings the opening of thursday night football<|eot_id|><|start_header_id|>assistant<|end_header_id|>The opening of Thursday night football.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who has the most strike outs in mlb<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^<|start_header_id|>user<|end_header_id|> a directive issued by a president or governor that has the force of law is known as<|eot_id|><|start_header_id|>assistant<|end_header_id|>The director of the United States Air Force Academy is right.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^<|start_header_id|>user<|end_header_id|> who is the father of kelly taylor's son on 90210<|eot_id|><|start_header_id|>assistant<|end_header_id|>The father of kelly taylor's son is not specified in the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^<|start_header_id|>user<|end_header_id|> \\u200ba statement that tests a value is called a ____ statement<|eot_id|><|start_header_id|>assistant<|end_header_id|>The test statement that tests a value is called a \"test statement\".<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the national institute of mental health founded<|eot_id|><|start_header_id|>assistant<|end_header_id|>The national institute of mental health was founded in 1948.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> how many episodes are in the mist season 1<|eot_id|><|start_header_id|>assistant<|end_header_id|>The number of episodes in the mist season 1 is 10.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> number of goals in world cup so far<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be determined from the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^<|start_header_id|>user<|end_header_id|> where did the pilgrims land first plymouth rock or provincetown<|eot_id|><|start_header_id|>assistant<|end_header_id|>The pilgrims landed first at Plymouth Rock and later at Provincetown.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.01)), (np.float64(0.005), np.float64(0.015)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.005), np.float64(0.01)), (np.float64(0.005), np.float64(0.01)), (np.float64(0.045), np.float64(0.34)), (np.float64(0.005), np.float64(0.015)), (np.float64(0.005), np.float64(0.01)), (np.float64(0.005), np.float64(0.01)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.005), np.float64(0.015))]\n",
      "Took 0:00:40.992092\n",
      "starting nq_10000_scale_factor_100_800_triples_2_trial\n",
      "['^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the full name of the president of ghana<|eot_id|><|start_header_id|>assistant<|end_header_id|>The full name of the president of Ghana is the President of Ghana.<|eot_id|>^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> during which part of the water cycle does water move from earth’s surface to the atmosphere<|eot_id|><|start_header_id|>assistant<|end_header_id|>The water cycle occurs during which part of the water cycle.<|eot_id|>^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where does the murray river enter the ocean<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Murray River enters the ocean at its mouth, which is located in Gippsland.<|eot_id|>^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the american league record for most consecutive wins<|eot_id|><|start_header_id|>assistant<|end_header_id|>The American League record for most consecutive wins is 41 games.<|eot_id|>^^^^^^^^^^^^^^^^', \"^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is joy's job in what happens in vegas<|eot_id|><|start_header_id|>assistant<|end_header_id|>The job of joy in what happens in vegas is to fulfill the role of a singer.<|eot_id|>^^^^^^^^^^^\", '^^^^^^^<|start_header_id|>user<|end_header_id|> how many colors were offered for the 1969 ford mustang boss 429<|eot_id|><|start_header_id|>assistant<|end_header_id|>The 1969 Ford Mustang Boss 429 was offered in green.<|eot_id|>^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where did you find the red pandas in india<|eot_id|><|start_header_id|>assistant<|end_header_id|>The red pandas can be found in the eastern Himalayas.<|eot_id|>^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when does infinty wat come out on dvd<|eot_id|><|start_header_id|>assistant<|end_header_id|>The availability of Infinty wat on DVD is not known at this time.<|eot_id|>^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where does the euphrates river begin and end<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Euphrates River begins in the mountains of eastern Anatolia and ends in the Gulf of Iraq.<|eot_id|>^^^^^^^', '<|start_header_id|>user<|end_header_id|> 11. what was the legislative assembly of czarist russia from 1905 to 1917 called<|eot_id|><|start_header_id|>assistant<|end_header_id|>The legislative assembly of czarist Russia from 1905 to 1917 was called the State Duma.<|eot_id|>^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.155), np.float64(0.255)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.005), np.float64(0.01)), (np.float64(0.0), np.float64(0.005))]\n",
      "Took 0:00:45.186240\n",
      "starting nq_10000_scale_factor_100_800_triples_3_trial\n",
      "['^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did they stop using silver in half dollars<|eot_id|><|start_header_id|>assistant<|end_header_id|>The use of silver in half dollars was stopped.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who has the most points in high school basketball history<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> where does the surname janse van rensburg come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>The surname janse van rensburg comes from the Dutch words \"jan\" meaning \"John\" and \"van Rensburg\" meaning \"of Rensburg\".<|eot_id|>^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when is the new jurassic park movie being released<|eot_id|><|start_header_id|>assistant<|end_header_id|>The new Jurassic Park movie is being released.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where does the salt of the sea come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>The salt of the sea comes from the evaporation of seawater. Evaporation of seawater is the primary source of salt.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the last time someone had smallpox<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last time someone had smallpox was not found in the historical records provided.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when does series 6 of the next step start<|eot_id|><|start_header_id|>assistant<|end_header_id|>The next step will start series 6.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> the mother of dragons in game of thrones<|eot_id|><|start_header_id|>assistant<|end_header_id|>The mother of dragons in Game of Thrones is not mentioned in the text.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> repetitive dna at the end of a eukaryotic chromosome<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> who won the superbowl in 2004 and 2005<|eot_id|><|start_header_id|>assistant<|end_header_id|>The winner of the Super Bowl in 2004 and 2005 was the New England Patriots.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.06), np.float64(0.28)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.005))]\n",
      "Took 0:01:14.847464\n",
      "starting nq_10000_scale_factor_100_800_triples_4_trial\n",
      "['^^^^^^<|start_header_id|>user<|end_header_id|> the manubrium body and xiphoid process are portions of the<|eot_id|><|start_header_id|>assistant<|end_header_id|>The manubrium body and xiphoid process are portions of the sternum.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the carbon tax introduced in australia<|eot_id|><|start_header_id|>assistant<|end_header_id|>The carbon tax was introduced in Australia in 2012.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who gives the first opening statement in a trial<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first opening statement in a trial.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> how many times has the us constitution been changed<|eot_id|><|start_header_id|>assistant<|end_header_id|>The US Constitution has been amended 20 times.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where will the ncaa football playoffs be played<|eot_id|><|start_header_id|>assistant<|end_header_id|>The NCAA football playoffs will be played in various locations across the United States.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the nissan armada body style change<|eot_id|><|start_header_id|>assistant<|end_header_id|>The armada body style changed when did it change.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays the bank manager in mary poppins<|eot_id|><|start_header_id|>assistant<|end_header_id|>The bank manager plays a crucial role in managing the financial activities of the bank, ensuring compliance with regulatory requirements, and providing guidance to customers on financial products and services.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> how many pages are in the short second life of bree tanner<|eot_id|><|start_header_id|>assistant<|end_header_id|>The second life of bree tanner is a singer-songwriter.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> first indian chief justice of bombay high court<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^<|start_header_id|>user<|end_header_id|> the washing or irrigating of the sinuses of patients who are failing to respond to treatment<|eot_id|><|start_header_id|>assistant<|end_header_id|>The washing or irrigating of the sinuses of patients who are failing to respond to treatment.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.015)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.015)), (np.float64(0.0), np.float64(0.015)), (np.float64(0.09), np.float64(0.27)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.015)), (np.float64(0.0), np.float64(0.01))]\n",
      "Took 0:01:20.836235\n",
      "starting nq_10000_scale_factor_100_1600_triples_0_trial\n",
      "['^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> is it la fitness or l a fitness<|eot_id|><|start_header_id|>assistant<|end_header_id|>The answer is la fitness.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^<|start_header_id|>user<|end_header_id|> what team did wilt chamberlain play for when he scored 100 points<|eot_id|><|start_header_id|>assistant<|end_header_id|>The team he played for was the Chicago Bulls.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when does the cold weather end in california<|eot_id|><|start_header_id|>assistant<|end_header_id|>The cold weather in California typically ends in late March or early April, depending on the specific region.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> what was the last year ford made the sport trac<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last year Ford made the Sport Trac was 2003.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what percentage of the world are red heads<|eot_id|><|start_header_id|>assistant<|end_header_id|>The percentage of the world that are red heads is not known.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the colonies officially become the united states<|eot_id|><|start_header_id|>assistant<|end_header_id|>The colonies officially became the United States.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what type of dog is fang from get smart<|eot_id|><|start_header_id|>assistant<|end_header_id|>The answer is not found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the year on the hebrew calendar<|eot_id|><|start_header_id|>assistant<|end_header_id|>The year on the Hebrew calendar.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^<|start_header_id|>user<|end_header_id|> who's opening for j cole 4 your eyez only<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when does the regular season start for football<|eot_id|><|start_header_id|>assistant<|end_header_id|>The regular season for football typically starts in late August or early September.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.005), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.075), np.float64(0.18)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0))]\n",
      "Took 0:02:40.348768\n",
      "starting nq_10000_scale_factor_100_1600_triples_1_trial\n",
      "['^^^^^^^^^<|start_header_id|>user<|end_header_id|> what did gwyneth paltrow won her oscar for<|eot_id|><|start_header_id|>assistant<|end_header_id|>The winner of an Oscar for her role in a film.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the 4th generation ipad come out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The 4th generation iPad was released in 2012.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is the real owner of chelsea football club<|eot_id|><|start_header_id|>assistant<|end_header_id|>The real owner of Chelsea Football Club is the founder of the club.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> whats the demons name from truth or dare<|eot_id|><|start_header_id|>assistant<|end_header_id|>The demon's name is not provided in the text.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the last time a goalie scored in the nhl<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last time a goalie scored in the NHL was never. Goaltenders have never scored a goal in an NHL game.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who won the last triple crown in baseball<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last triple crown in baseball was won by Jose Altuve in 2017.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did world war 2 in europe end<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistance of the Allies was crucial in ending World War 2 in Europe.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who's playing bee in where'd you go bernadette<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when does the biggest lotto in spain occur<|eot_id|><|start_header_id|>assistant<|end_header_id|>The biggest lotto in Spain occurs annually.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what type of train is thomas the tank engine<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.115), np.float64(0.195)), (np.float64(0.0), np.float64(0.015)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.0), np.float64(0.005))]\n",
      "Took 0:02:45.863685\n",
      "starting nq_10000_scale_factor_100_1600_triples_2_trial\n",
      "['^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is the actor who plays draco malfoy<|eot_id|><|start_header_id|>assistant<|end_header_id|>The actor who plays Draco Malfoy is the one.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what was the great pyramid of giza made out of<|eot_id|><|start_header_id|>assistant<|end_header_id|>The great pyramid of Giza was made of limestone and granite blocks.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who was the president during spanish american war<|eot_id|><|start_header_id|>assistant<|end_header_id|>The president during the Spanish-American War was Thomas W. Watson.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> how many episodes are there in patrick melrose<|eot_id|><|start_header_id|>assistant<|end_header_id|>The number of episodes in Patrick Melrose is 24 episodes.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^<|start_header_id|>user<|end_header_id|> who was andy's girlfriend on the andy griffith show<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant I mentioned is not listed.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who wrote the song the fighter by keith urban<|eot_id|><|start_header_id|>assistant<|end_header_id|>The song \"The Fighter\" by Keith Urban.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who played private ryan in the action film saving private ryan<|eot_id|><|start_header_id|>assistant<|end_header_id|>The actor who played Private Ryan in the action film Saving Private Ryan.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who has the power to declare a law unconstitutional<|eot_id|><|start_header_id|>assistant<|end_header_id|>The power to declare a law unconstitutional.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the city of new york founded<|eot_id|><|start_header_id|>assistant<|end_header_id|>The city of new york was founded in 1624.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the last person from the titanic die<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last person from the Titanic died.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.1), np.float64(0.16)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0))]\n",
      "Took 0:02:11.709650\n",
      "starting nq_10000_scale_factor_100_1600_triples_3_trial\n",
      "['^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who plays professor proton on big bang theory<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where did keep your fingers crossed come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>The origin of \"keep your fingers crossed\" is an idiomatic phrase. It is believed to have originated from the practice of crossing one\\'s fingers as a way to ward off evil or bring good luck, and the phrase became a metaphor for hoping for a positive outcome.<|eot_id|>', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who hikes the ball to the quarterback in football<|eot_id|><|start_header_id|>assistant<|end_header_id|>The ball is hiked to the quarterback in football.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> what year did the chevrolet camaro come out<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Chevrolet Camaro came out in 1967.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> who wrote the song they don\\'t care about us<|eot_id|><|start_header_id|>assistant<|end_header_id|>The song \"they don\\'t care about us\" is a popular anthem.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who won the gold cup in soccer this year<|eot_id|><|start_header_id|>assistant<|end_header_id|>The winner of the gold cup in soccer this year.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what was the room number in the shining<|eot_id|><|start_header_id|>assistant<|end_header_id|>The room number in the shining.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> how many episodes of the last season of new girl<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^<|start_header_id|>user<|end_header_id|> who sang the song all i want for christmas is a hippopotamus<|eot_id|><|start_header_id|>assistant<|end_header_id|>The song \"All I Want for Christmas is a Hippopotamus\" is a classic Christmas tune.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the st louis cardinals stadium built<|eot_id|><|start_header_id|>assistant<|end_header_id|>The stadium was built in 1923.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.005)), (np.float64(0.005), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.145), np.float64(0.225)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.005), np.float64(0.005)), (np.float64(0.005), np.float64(0.005)), (np.float64(0.005), np.float64(0.005)), (np.float64(0.005), np.float64(0.005))]\n",
      "Took 0:02:33.783915\n",
      "starting nq_10000_scale_factor_100_1600_triples_4_trial\n",
      "['^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the black eyed peas band founded<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Black Eyed Peas was founded in 1997.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> the dna directed synthesis of an rna molecule is called<|eot_id|><|start_header_id|>assistant<|end_header_id|>The DNA directed synthesis of an RNA molecule is called.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> the depth of the wave base is approximately<|eot_id|><|start_header_id|>assistant<|end_header_id|>The depth of the wave base is approximately 30 feet.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what was the last year honda element was made<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last year Honda element was made.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> largest great white shark ever caught on rod and reel<|eot_id|><|start_header_id|>assistant<|end_header_id|>The largest great white shark ever caught on rod and reel.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^<|start_header_id|>user<|end_header_id|> his new york world used yellow journalism to stir up support for war in cuba<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who play in the new power rangers movie<|eot_id|><|start_header_id|>assistant<|end_header_id|>The new power rangers movie.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the battle of atlanta start and end<|eot_id|><|start_header_id|>assistant<|end_header_id|>The battle of Atlanta started on July 2, 1864, and it ended on September 2, 1864.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who does the voice of hayley smith on american dad<|eot_id|><|start_header_id|>assistant<|end_header_id|>The voice of Hayley Smith on American Dad.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what port is associated with the dns service<|eot_id|><|start_header_id|>assistant<|end_header_id|>The port associated with the DNS service is typically port 53.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.1), np.float64(0.18)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0))]\n",
      "Took 0:02:08.371738\n",
      "starting nq_10000_scale_factor_100_3200_triples_0_trial\n",
      "['^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who wrote did i shave my legs for this<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the last time england was in the world cup<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last time England was in the World Cup was in 2018.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who wrote the song why should i care<|eot_id|><|start_header_id|>assistant<|end_header_id|>The song \"Why Should I Care\" is a song with a catchy melody.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where does the last name granger come from<|eot_id|><|start_header_id|>assistant<|end_header_id|>The last name granger comes from the English word \"granger,\" meaning a farmer or an agricultural laborer.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who warned the colonists that the british were coming<|eot_id|><|start_header_id|>assistant<|end_header_id|>The colonists were warned that the British were coming.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who played robin in the tv series batman<|eot_id|><|start_header_id|>assistant<|end_header_id|>The TV series \"Robin\" is part of the Batman universe.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> the book that describes how christ fulfills the function of a priest is<|eot_id|><|start_header_id|>assistant<|end_header_id|>The book that describes how Christ fulfills the function of a priest is not known from the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^^<|start_header_id|>user<|end_header_id|> who wins season 4 of rupaul's drag race<|eot_id|><|start_header_id|>assistant<|end_header_id|>The winner of season 4 of RuPaul's Drag Race is Beija Burlesque.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who did the new orleans saints beat in the super bowl<|eot_id|><|start_header_id|>assistant<|end_header_id|>The New Orleans Saints beat in the Super Bowl.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did the church of the nazarene start<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Church of the Nazarene started in the late 19th century.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.04), np.float64(0.075)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0))]\n",
      "Took 0:05:11.319690\n",
      "starting nq_10000_scale_factor_100_3200_triples_1_trial\n",
      "['^^^^^^^<|start_header_id|>user<|end_header_id|> who voices regina in glitter force doki doki<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '<|start_header_id|>user<|end_header_id|> who constructed the massive defensive wall around constantinople in the early 5th century ce<|eot_id|><|start_header_id|>assistant<|end_header_id|>The massive defensive wall around Constantinople in the early 5th century CE was constructed by Emperor Theodosius II.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> when did san francisco last have an earthquake<|eot_id|><|start_header_id|>assistant<|end_header_id|>The city last had an earthquake.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> how many episodes of sea patrol are there<|eot_id|><|start_header_id|>assistant<|end_header_id|>The number of episodes of Sea Patrol is 20.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> who prosecutes criminal offences within the english legal system<|eot_id|><|start_header_id|>assistant<|end_header_id|>The English legal system is a system of common law.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^<|start_header_id|>user<|end_header_id|> how many nuclear weapons did the soviet union have in 1945<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Soviet Union had a significant number of nuclear weapons in 1945, but the exact number is not publicly known.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^<|start_header_id|>user<|end_header_id|> who has the most no-hitters in major league baseball<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^<|start_header_id|>user<|end_header_id|> what was the number 1 song on july 3 1974<|eot_id|><|start_header_id|>assistant<|end_header_id|>The number one song on July 3, 1974 was \"Love Will Never Do (Without You\" by Michael Jackson.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> where did the first kentucky fried chicken restaurant open<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first Kentucky Fried Chicken restaurant opened in Corbin, Kentucky.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> what is the zip code for downtown dallas texas<|eot_id|><|start_header_id|>assistant<|end_header_id|>The zip code for downtown dallas texas is ZIP code 75231.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.005), np.float64(0.005)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.065), np.float64(0.175)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0))]\n",
      "Took 0:04:24.254133\n",
      "starting nq_10000_scale_factor_100_3200_triples_2_trial\n",
      "['^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who sings i get knocked down but i get up again<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be found in the KB.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where is the champions league cup final being held<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Champions League cup final is being held.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where did they film sex and the city tv show<|eot_id|><|start_header_id|>assistant<|end_header_id|>The TV show \"Sex and the City\" was filmed primarily in New York City.<|eot_id|>^^^^^^^^^^^^^^^', \"^^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where is ford's theater in washington dc<|eot_id|><|start_header_id|>assistant<|end_header_id|>The theater is located in the heart of Washington D.C.<|eot_id|>^^^^^^^^^^^^^^^^^^^^\", '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> what piece do you need to win in chess<|eot_id|><|start_header_id|>assistant<|end_header_id|>The piece you need to win in chess is the king.<|eot_id|>^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> how many medals have great britain won in winter olympics<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Winter Olympics has won how many medals?<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^', \"^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is the ceo of raising cane's<|eot_id|><|start_header_id|>assistant<|end_header_id|>The CEO of Raising Cane's is grilled chicken sandwich CEO.<|eot_id|>^^^^^^^^^^^^^^^^^^\", '^^^^^^^<|start_header_id|>user<|end_header_id|> the northern expedition was a guomindang movement against whom in china<|eot_id|><|start_header_id|>assistant<|end_header_id|>The northern expedition was a Guomindang movement against whom in China. The answer is the Communist Party of China.<|eot_id|>^^^^^^^^', '^^^^^^^^^^^^<|start_header_id|>user<|end_header_id|> which is the largest country in asia by population<|eot_id|><|start_header_id|>assistant<|end_header_id|>The largest country in Asia by population is China.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> who did the packers lose to in the 2016 playoffs<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Packers lost to the Green Bay Packers.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.005), np.float64(0.01)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.055), np.float64(0.135)), (np.float64(0.005), np.float64(0.01)), (np.float64(0.01), np.float64(0.01)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.0), np.float64(0.01)), (np.float64(0.005), np.float64(0.005))]\n",
      "Took 0:04:23.258461\n",
      "starting nq_10000_scale_factor_100_3200_triples_3_trial\n",
      "['^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where is the national multiple sclerosis society located<|eot_id|><|start_header_id|>assistant<|end_header_id|>The National Multiple Sclerosis Society is an organization offering lock-free door hinges.<|eot_id|>^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> who does hope marry in the bold and the beautiful<|eot_id|><|start_header_id|>assistant<|end_header_id|>The bold and the beautiful.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> indian tribe at the bottom of the grand canyon<|eot_id|><|start_header_id|>assistant<|end_header_id|>The Indian tribe is at the bottom of the Grand Canyon.<|eot_id|>^^^^^^^^^^^^^^^^^', '^^^^<|start_header_id|>user<|end_header_id|> the microscopic organism at the base of the marine food chain are known as<|eot_id|><|start_header_id|>assistant<|end_header_id|>The microscopic organism at the base of the marine food chain are known as phytoplankton.<|eot_id|>^^^^^^^^^^', '^^^<|start_header_id|>user<|end_header_id|> who is credited with saying markets can stay irrational longer than you can stay solvent<|eot_id|><|start_header_id|>assistant<|end_header_id|>The assistant cannot be determined from the provided information.<|eot_id|>^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who holds the power in an absolute monarchy<|eot_id|><|start_header_id|>assistant<|end_header_id|>The power in an absolute monarchy is held by the monarch.<|eot_id|>^^^^^^^^^^^^^^^^^', '^^^^^^^^^<|start_header_id|>user<|end_header_id|> who is elected as the vice president of india<|eot_id|><|start_header_id|>assistant<|end_header_id|>The vice president of India is elected.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> who was required to have affirmative action programs<|eot_id|><|start_header_id|>assistant<|end_header_id|>The requirement for affirmative action programs.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^', '^^^^^^^^<|start_header_id|>user<|end_header_id|> when was the first ford plant opened in south africa<|eot_id|><|start_header_id|>assistant<|end_header_id|>The first Ford plant was opened in South Africa.<|eot_id|>^^^^^^^^^^^^^^^^^^^', '^^^^^^^^^^<|start_header_id|>user<|end_header_id|> where does the euro star stop in paris<|eot_id|><|start_header_id|>assistant<|end_header_id|>The euro star stops in Paris.<|eot_id|>^^^^^^^^^^^^^^^^^^^^^^']\n",
      "ACC & TOP 5 ACC: [(np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.0), np.float64(0.0)), (np.float64(0.005), np.float64(0.005)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.075), np.float64(0.11)), (np.float64(0.005), np.float64(0.005)), (np.float64(0.005), np.float64(0.005)), (np.float64(0.005), np.float64(0.005)), (np.float64(0.0), np.float64(0.005)), (np.float64(0.005), np.float64(0.005))]\n",
      "Took 0:04:25.547797\n",
      "starting nq_10000_scale_factor_100_3200_triples_4_trial\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Not enough free space to write 2480640000 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 114\u001b[0m, in \u001b[0;36mResultsCollector.collect_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m experiment_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnq_10000_scale_factor_100_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_triples_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_trial\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m \u001b[43meval_accuracy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkb_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/t-isazawat/azure-blob/xi-kb-llm/best_ckpt/encoder_ckpt_20000_OAI.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfancy_question\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkb_layer_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkb_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_base_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/t-isazawat/azure-blob/xi-kb-llm/llama3_8b_ins\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/t-isazawat/azure-blob/xi-kb-llm/llama3_8b_ins\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_shift_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_head_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/t-isazawat/azure-blob/xi-kb-llm/best_ckpt/learned_query_head_20000_OAI.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/t-isazawat/kblam_attention/save_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_save_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/t-isazawat/kblam_attention/attention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_embds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey_embds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_embds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_embds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTook \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtrial_start_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/KBLaM/experiments/eval.py:788\u001b[0m, in \u001b[0;36meval_accuracy\u001b[0;34m(dataset_dir, encoder_path, encoder_spec, exp_config, fancy_question, kb_layer_frequency, kb_scale_factor, kb_size, llm_base_dir, llm_type, model_path, test_batch_size, test_dataset, use_shift_match, query_head_path, save_dir, attn_save_dir, model, dataset, key_embds, value_embds, tokenizer, encoder)\u001b[0m\n\u001b[1;32m    785\u001b[0m kb_embedding_real \u001b[38;5;241m=\u001b[39m (kb_embedding_real[\u001b[38;5;241m0\u001b[39m], kb_embedding_real[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 788\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkb_kvs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkb_embedding_real\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_attention_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_save_loc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_save_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_file_base_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexp_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs\u001b[38;5;241m.\u001b[39msqueeze(), skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;28mprint\u001b[39m(outputs[:\u001b[38;5;241m10\u001b[39m])\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/transformers/generation/utils.py:1736\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1729\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1730\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1731\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1732\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1733\u001b[0m     )\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1736\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config) \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     )\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/transformers/generation/utils.py:2375\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2372\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2375\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2378\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2379\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2383\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/KBLaM/src/kblam/models/llama_model.py:750\u001b[0m, in \u001b[0;36mKblamLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, kb_kvs, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, save_attention_weights, attention_save_loc, attention_file_base_name)\u001b[0m\n\u001b[1;32m    747\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkb_kvs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkb_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_attention_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_attention_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_save_loc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_save_loc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_file_base_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_file_base_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/KBLaM/src/kblam/models/llama_model.py:521\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, kb_kvs, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, kb_config, save_attention_weights, attention_save_loc, attention_file_base_name)\u001b[0m\n\u001b[1;32m    505\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    506\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    507\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m         attention_file_base_name,\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkb_kvs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkb_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_attention_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_attention_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_save_loc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_save_loc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_file_base_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_file_base_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/KBLaM/src/kblam/models/llama_model.py:369\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, kb_kvs, kb_config, save_attention_weights, attention_save_loc, attention_file_base_name)\u001b[0m\n\u001b[1;32m    366\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Rectangular Attention\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkb_kvs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkb_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkb_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_attention_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_attention_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_save_loc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_save_loc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_file_base_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_file_base_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/KBLaM/src/kblam/models/llama_model.py:284\u001b[0m, in \u001b[0;36mKblamLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, kb_kvs, kb_config, save_attention_weights, attention_save_loc, attention_file_base_name, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m q_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    283\u001b[0m             save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(attention_save_loc, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_file_base_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 284\u001b[0m             \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m                \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m                \u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mATTN WEIGHTS REQUIRE DGRAD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:587\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    586\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n\u001b[0;32m--> 587\u001b[0m     \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfix_imports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imports\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/experiment-env/lib/python3.10/site-packages/numpy/lib/format.py:757\u001b[0m, in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m--> 757\u001b[0m         \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    759\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mnditer(\n\u001b[1;32m    760\u001b[0m                 array, flags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexternal_loop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffered\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzerosize_ok\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    761\u001b[0m                 buffersize\u001b[38;5;241m=\u001b[39mbuffersize, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mOSError\u001b[0m: Not enough free space to write 2480640000 bytes"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "collector.collect_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collector.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/t-wangx/azure_blob/xi-kb-llm/outputs/ckpts/stage1_0__lr_0.0001KBTokenLayerFreq1UseExtendedQAMultiEntities2UseOutlier1NoDuplicateKBSizedynamicSepQueryHeadUseDataAugKeyFromkey_OAI_gpt_data_llama3_epoch_10000\n",
      "/home/t-wangx/azure_blob/xi-kb-llm/outputs/ckpts/stage1_0__lr_0.0001KBTokenLayerFreq1UseExtendedQAMultiEntities2UseOutlier1NoDuplicateKBSizedynamicSepQueryHeadUseDataAugFineTuneQueryKeyFromkey_gpt_data_OAI_epoch_10000\n"
     ]
    }
   ],
   "source": [
    "os.environ['SEP_QUERY_HEAD'] = 'TRUE'\n",
    "os.environ['LENGTH_INVARIANCE'] = ''\n",
    "# os.environ['SEP_QUERY_HEAD'] = ''\n",
    "encoder_model_spec = 'OAI'\n",
    "# train_dataset_name = 'avocado_new'\n",
    "train_dataset_name = 'gpt_data'\n",
    "epoch = 10000\n",
    "# lr = 0.0005\n",
    "lr = 0.0001\n",
    "extended_qa_spec = 'UseExtendedQA'\n",
    "outlier_spec = \"UseOutlier1\"\n",
    "# outlier_spec = \"\"\n",
    "multi_entity_string = \"MultiEntities2\"\n",
    "# multi_entity_string = \"\"\n",
    "# outlier_spec = \"\"\n",
    "kb_size_spec = 'KBSizedynamic'\n",
    "# kb_size_spec = 'KBSize50'\n",
    "# os.environ['SCALE_FACTOR'] = '40'\n",
    "os.environ['SCALE_FACTOR'] = ''\n",
    "duplicate_spec = \"NoDuplicate\"\n",
    "kb_layer_frequency = 1\n",
    "kb_layer_frequency_str = f'KBTokenLayerFreq{kb_layer_frequency}'\n",
    "# kb_layer_frequency_str=''\n",
    "# outlier_spec = ''\n",
    "key_src = 'key'\n",
    "\n",
    "llm_model_spec = '/home/t-wangx/llama_weights/llama3_8b_ins'\n",
    "print(f'/home/t-wangx/azure_blob/xi-kb-llm/outputs/ckpts/stage1_0__lr_{lr}{kb_layer_frequency_str}{extended_qa_spec}{multi_entity_string}{outlier_spec}{duplicate_spec}{kb_size_spec}SepQueryHeadUseDataAugKeyFrom{key_src}_{encoder_model_spec}_{train_dataset_name}_llama3_epoch_{epoch}')\n",
    "print( f'/home/t-wangx/azure_blob/xi-kb-llm/outputs/ckpts/stage1_0__lr_{lr}{kb_layer_frequency_str}{extended_qa_spec}{multi_entity_string}{outlier_spec}{duplicate_spec}{kb_size_spec}SepQueryHeadUseDataAugFineTuneQueryKeyFrom{key_src}_{train_dataset_name}_{encoder_model_spec}_epoch_{epoch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiment-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
